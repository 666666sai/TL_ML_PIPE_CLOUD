How to train a model

                                                   predicted value
                                                           ^
                                                           |
                                                           |
                                                           |
        training set-----> learning algoritham ------->hypothesis function
                                                           |
                                                           |
                                                           |
                                                   x continuous value


Classification aims to predict a discrete, categorical label (e.g., dog, cat, or Class A, Class B).

Regression aims to predict a continuous, numerical value (e.g., 45.7, 100,000).


Linear Regression,                  Regression,--------------"Predicts a continuous output value (e.g., house price, temperature) by fitting a straight line (or hyperplane) to the data."
Logistic Regression,                Classification,----------"Predicts the probability of an instance belonging to a specific class (e.g., Yes/No, Spam/Not Spam). It's primarily used for binary classification."
Support Vector Machines (SVM),      Classification------------(and Regression: SVR),"Finds the optimal hyperplane that best separates different classes in the feature space, maximizing the margin between them."
Naive Bayes,                        Classification,-----------"A probabilistic classifier based on Bayes' theorem, assuming that features are independent of each other given the class."
Decision Trees,                     Both,-------------------- Uses a tree-like structure of decisions (based on features) to predict an output. Can handle both continuous (regression) and categorical (classification) data.
K-Nearest Neighbors (KNN),          Both,---------------------"A non-parametric, instance-based model that classifies a new data point based on the majority class of its 'k' nearest neighbors in the feature space."
Gradient Boosting,                  Both,--------------------"An ensemble technique that builds models sequentially, where each new model corrects the errors of the previous ones. Includes models like XGBoost and LightGBM."
Random Forests,                     Both,---------------------An ensemble method that builds multiple Decision Trees and merges their predictions to get a more accurate and stable result. Excellent for both tasks.


1. The Hierarchy
        Feature: The Question.
        Category: The Options for the answer.
        Class: The Result/Grade at the end.
        Types of Naive Bayes Models (and When to Use Each)

Term,                   Symbol,                 Role,                                Definition,                                           Example
Feature,                X (Column),             The Input,              An attribute or property of the data you are observing.,        """Color"""
Category,               Xvalueâ€‹,                 The Input Value,        The specific distinct groups inside a Feature.,                """Red"", ""Green"", ""Blue"""
Class,                  y (Target),             The Output,             The final label or answer the model tries to predict.,         """Apple"" vs. ""Banana"""



Linear, logistic, random forest , decision tree, 
pca, ensemble techniques like xg , gradient boosts, 
smote  l1 l2 pruning, light , ada 
svm, naive bayes....oversampling , undersampling techs, over fitting